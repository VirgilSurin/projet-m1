# problems
This repo contains all the algorithmic problems that CPUMons uses for its training courses. Those problems are well formated to be used with the DOMtutor system.

# installing
 * In order to install your own instance of DOMtutor (in local)
   * go to `./domtutor-local` and follow the ![README](./domtutor-local/README.md)
 * Create a virtual environment with `python3 -m venv venv` and activate it with `. ./venv/bin/activate`
 * clone those repo :
   * https://github.com/DOMtutor/problemtools
   * https://github.com/DOMtutor/pyjudge
 * `pip install <a lot of things>` (There are some requirements.txt files which aren't completely up to date ...)
   * `plastex`
 * Setting up Kattis problemtools - Inside `problemtools`
   * `git submodule init --update --recursive`
   * `make` (may need `libgmp-dev`) to compile `checktestdata`
   * `pip install -e ./` to install problemtools into the venv
 * Installing pyjudge in the venv: `cd pyjudge && pip install -e ./`
 * To build problems you may also need `texlive-latex-extra poppler-utils` (or your distribution's equivalent)
 
# config.yaml
Contains some configurations and informations. Mainly all the authors of problem.

# setting up a problem
All problem goes to `./repository/problems/`
Some example problem :
  - helloworld (a classic one)
  - guess (an interactive problem)
  - sand (for input validation)
  
You can use the script `create_problem.py` to auto-generate a default problem structure with a given name.
Usage : `python create_problem.py your_problem_name`
## Problem structure
A problem has a defined structure (mandatory file are marked with a `*`) :
  - (*) problem.yaml (a description of the problem)
  - (*) data (the examples and secrets test cases)
  -  generators (a script that given some parameters, generates a random input for the problem)
- (*) input\_validators/ (contains a script that accept a valid problem input, see /repository/problems/sand/input_validators/ for an example)
- output_validators/ (same but to check output, aka correct formatting of a solution)
  - (*) problem_statement/ (contains .tex used to generate the pdf of the problem)
  - (*) submissions/
    - (*) /accepted/ (accepted submissions a.k.a jury's solutions)
  - build/ (where the problem's pdf is placed when created)
  - solution/ (a .tex for generating a solution slide, not that useful)
  
For the `/data/`, you can have `/data/sample/` and `/data/secret/` respectively for the data you want to give as example and for for the secret test cases.

A test case is composed by at most 3 files. The `.in`, `.ans` and `.seed`. 
The `.in` contains the input that will be given to the submission.
`.ans` is the corresponding expected solution (what the submission should print).
The `.seed` is used to feed the code in `/generator/`. It will, if no corresponding `.in` exists, generate it based on what's in the `.seed`.
Note that for each test cases, if there is only a `.seed`, it will generate a corresponding `.in` and then a `.ans` using one of the accepted submissions.
If only a `.in` is present, it will generate a corresponding `.ans` using one of the accepted submissions.

NOTE: the .ans and .in will be generated by the `contest` command described in the [Using the Upload Script](./README.md#Using-the-Upload-Script) section.

# how to configure your contest
The file you are looking for is `contest.json`.
First you need to understand that there can be multiple contest running on domjudge with different problem set and users/teams. Those contest are differenciated by their `key`.
They can share the same `name`, it's just for display purpose.

There are 4 scheduling parameters :
 * activate : from when users can log into the contest and wait
 * start : when the contest start and the problem set is accessible
 * end : the contest is over but you can still submit
 * deactivate : the contest is not accessible anymore
 The date format is as follow : `yyyy-mm-ddThh:mm:ss Europe/Vienna`
 
 Then there is the problem list.
 A problem is defined by a `name`, a number of point `points` given when solved, a color and `problem` the problem name in `/repository/problems/`.

# Using the Upload Script

 * Adapt the json files as needed; add/change the problems in repository
 * The `<<judge_upload>>` command now means `judge_upload --db db.yml --instance instance.json --repository repository <command>`
 * Sync settings by `<<judge_upload>> settings` (settings are specified through `instance.json`)
 * Sync users by `<<judge_upload>> users users.json` (the passwords of each user is the user name)
 * Sync contest by `<<judge_upload>> contest contest.json --force` (`--force` since the contest is currently running)
 

## Automation

The repo also contains a script which demonstrates how to obtain these files from python objects (i.e. how you can use DOMtutor to automate your setup)
Run `python script.py --pretty settings|users|contest` to create the output.
 * to update user list from a Moodle group export (xlsx) use `python script.py --input ./path_to_file.xlsx -O ./users.json export`
